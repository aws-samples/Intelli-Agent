from enum import Enum

QA_SEP = "=>"
AWS_Free_Chat_Prompt = """你是云服务AWS的智能客服机器人{B}，能够回答{A}的各种问题以及陪{A}聊天，如:{chat_history}\n\n{A}: {question}\n{B}: """
AWS_Knowledge_QA_Prompt = """你是云服务AWS的智能客服机器人{B}，请严格根据反括号中的资料提取相关信息\n```\n{fewshot}\n```\n回答{A}的各种问题，比如:\n\n{A}: {question}\n{B}: """
Fewshot_prefix_Q="问题"
Fewshot_prefix_A="回答"

class QueryType(Enum):
    KeywordQuery   = "KeywordQuery"       #用户仅仅输入了一些关键词（2 token)
    KnowledgeQuery = "KnowledgeQuery"     #用户输入的需要参考知识库有关来回答
    Conversation   = "Conversation"       #用户输入的是跟知识库无关的问题

def combine_recalls(opensearch_knn_respose, opensearch_query_response):
    '''
    filter knn_result if the result don't appear in filter_inverted_result
    '''
    knn_threshold = 0.2
    inverted_theshold = 5.0
    filter_knn_result = { item["doc"] : item["score"] for item in opensearch_knn_respose if item["score"]> knn_threshold }
    filter_inverted_result = { item["doc"] : item["score"] for item in opensearch_query_response if item["score"]> inverted_theshold }

    combine_result = []
    for doc, score in filter_knn_result.items():
        if doc in filter_inverted_result.keys():
            combine_result.append({ "doc" : doc, "score" : score })

    return combine_result

def concat_recall_knowledge(recall_knowledge_list):
    """
    Concat recall knowledge result from OpenSearch into a single string.
    """
    return "\n".join([item["doc"] for item in recall_knowledge_list])

def build_conversation_prompt(post_text, conversations, role_a, role_b):
    """
    Build conversation prompt for LLM.
    In current version, we concatenate all conversation history into a single prompt.

    :param post_text: user post text
    :param conversations: conversation history
    :param role_a: role name, e.g. "用户"
    :param role_b: role name, e.g. "AWSBot"
    :return: conversation prompt string
    """
    chat_history = [f"{role_a}: {item[0]}\n{role_b}: {item[1]}" for item in conversations]
    chat_histories = "\n\n".join(chat_history)
    chat_histories = f"\n\n{chat_histories}" if chat_histories else ""

    conversation_prompt = AWS_Free_Chat_Prompt.format(chat_history=chat_histories, question=post_text, A=role_a, B=role_b)
    
    return conversation_prompt

def build_knowledge_qa_prompt(post_text, qa_recalls, role_a, role_b):
    """
    build prompt using qa for LLM. 
    For Knowledge QA, it will merge all retrieved related document paragraphs into a single prompt
    
    :param post_text: user post text
    :param qa_recalls: all retrieved related document paragraphs from OpenSearch
    :param role_a: role name, e.g. "用户"
    :param role_b: role name, e.g. "AWSBot"
    """
    qa_pairs = [ obj["doc"].split(QA_SEP) for obj in qa_recalls ]
    qa_fewshots = [f"{Fewshot_prefix_Q}: {pair[0]}\n{Fewshot_prefix_A}: {pair[1]}" for pair in qa_pairs]
    fewshots_str = "\n\n".join(qa_fewshots[-3:])

    knowledge_qa_prompt = AWS_Knowledge_QA_Prompt.format(fewshot=fewshots_str, question=post_text, A=role_a, B=role_b)
    return knowledge_qa_prompt

def build_final_prompt(query_input, session_history, exactly_match_result, recall_knowledge, role_a, role_b):
    """
    built final prompt for generating answer for user LLM.

    :param query_input: user post text
    :param session_history: conversation history from DynamoDB
    :param exactly_match_result: exactly match result from OpenSearch
    :param recall_knowledge: knowledge recall result from OpenSearch
    :param role_a: role name, e.g. "用户"
    :param role_b: role name, e.g. "AWSBot"

    :return: (answer, final_prompt, query_type)
    """
  
    answer = None
    final_prompt = None
    query_type = None

    if exactly_match_result and recall_knowledge:
        query_type = QueryType.KeywordQuery
        answer = exactly_match_result[0]["doc"]
        final_prompt = ""
    elif recall_knowledge:
        query_type = QueryType.KnowledgeQuery
        final_prompt = build_knowledge_qa_prompt(query_input, recall_knowledge, role_a=role_a, role_b=role_b)
    else:
        query_type = QueryType.Conversation
        free_chat_coversions = [item for item in session_history if item[2] == QueryType.Conversation]
        final_prompt = build_conversation_prompt(query_input, free_chat_coversions[-2:], role_a=role_a, role_b=role_b)

    return (answer, final_prompt, query_type)